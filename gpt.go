package main

import (
	"bufio"
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strconv"
	"strings"
	"time"

	"github.com/gin-gonic/gin"
)

// https://github.com/sashabaranov/go-openai/blob/master/chat.go

const (
	InitialScannerBufferSize = 1 << 20  // 1MB (1*1024*1024)
	MaxScannerBufferSize     = 10 << 20 // 10MB (10*1024*1024)
	DefaultPingInterval      = 10 * time.Second
)

type GptModel struct {
	ID      string `json:"id"`
	Object  string `json:"object"`
	Created int    `json:"created"`
	OwnedBy string `json:"owned_by"`
}

// List represents the entire structure of the given JSON.
type GptModelListResp struct {
	Object string     `json:"object"`
	Data   []GptModel `json:"data"`
}
type Message struct {
	Role    string      `json:"role"`
	Content interface{} `json:"content"`
}
type ChatGPTRequest struct {
	Messages []Message `json:"messages"`
	Model    string    `json:"model"`
	Steam    bool      `json:"steam"`
}

type ChatGPTResponse struct {
	ID                string      `json:"id"`
	Object            string      `json:"object"`
	Created           int64       `json:"created"`
	Model             string      `json:"model"`
	SystemFingerprint string      `json:"system_fingerprint"`
	Choices           []GPTChoice `json:"choices"`
	Usage             Usage       `json:"usage"`
}

type ChatGPTSteanResponse struct {
	V string `json:"v"`
	P string `json:"p,omitempty"`
	O string `json:"o,omitempty"`
}

type ToolType string
type FinishReason string

type ChatMessagePartType string
type ImageURLDetail string
type ChatCompletionResponseFormatType string

type httpHeader http.Header

// Chat message role defined by the OpenAI API.
const (
	ChatMessageRoleSystem    = "system"
	ChatMessageRoleUser      = "user"
	ChatMessageRoleAssistant = "assistant"
	ChatMessageRoleFunction  = "function"
	ChatMessageRoleTool      = "tool"
	ChatMessageRoleDeveloper = "developer"
)

const (
	sseObject = "chat.completion.chunk"
)

const (
	FinishReasonStop          FinishReason = "stop"
	FinishReasonLength        FinishReason = "length"
	FinishReasonFunctionCall  FinishReason = "function_call"
	FinishReasonToolCalls     FinishReason = "tool_calls"
	FinishReasonContentFilter FinishReason = "content_filter"
	FinishReasonNull          FinishReason = "null"
)

type ChatMessageImageURL struct {
	URL    string         `json:"url,omitempty"`
	Detail ImageURLDetail `json:"detail,omitempty"`
}

type ChatMessagePart struct {
	Type     ChatMessagePartType  `json:"type,omitempty"`
	Text     string               `json:"text,omitempty"`
	ImageURL *ChatMessageImageURL `json:"image_url,omitempty"`
}

type ChatCompletionMessage struct {
	Role         string      `json:"role"`
	Content      interface{} `json:"content,omitempty"`
	Refusal      string      `json:"refusal,omitempty"`
	MultiContent []ChatMessagePart

	// This property isn't in the official documentation, but it's in
	// the documentation for the official library for python:
	// - https://github.com/openai/openai-python/blob/main/chatml.md
	// - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
	Name string `json:"name,omitempty"`

	// This property is used for the "reasoning" feature supported by deepseek-reasoner
	// which is not in the official documentation.
	// the doc from deepseek:
	// - https://api-docs.deepseek.com/api/create-chat-completion#responses
	ReasoningContent string `json:"reasoning_content,omitempty"`

	FunctionCall *FunctionCall `json:"function_call,omitempty"`

	// For Role=assistant prompts this may be set to the tool calls generated by the model, such as function calls.
	ToolCalls []ToolCall `json:"tool_calls,omitempty"`

	// For Role=tool prompts this should be set to the ID given in the assistant's prior request to call a tool.
	ToolCallID string `json:"tool_call_id,omitempty"`
}

type ChatCompletionResponseFormatJSONSchema struct {
	Name        string         `json:"name"`
	Description string         `json:"description,omitempty"`
	Schema      json.Marshaler `json:"schema"`
	Strict      bool           `json:"strict"`
}

type ChatCompletionResponseFormat struct {
	Type       ChatCompletionResponseFormatType        `json:"type,omitempty"`
	JSONSchema *ChatCompletionResponseFormatJSONSchema `json:"json_schema,omitempty"`
}
type FunctionDefinition struct {
	Name        string `json:"name"`
	Description string `json:"description,omitempty"`
	Strict      bool   `json:"strict,omitempty"`
	// Parameters is an object describing the function.
	// You can pass json.RawMessage to describe the schema,
	// or you can pass in a struct which serializes to the proper JSON schema.
	// The jsonschema package is provided for convenience, but you should
	// consider another specialized library if you require more complex schemas.
	Parameters any `json:"parameters"`
}

type Tool struct {
	Type     ToolType            `json:"type"`
	Function *FunctionDefinition `json:"function,omitempty"`
}
type StreamOptions struct {
	// If set, an additional chunk will be streamed before the data: [DONE] message.
	// The usage field on this chunk shows the token usage statistics for the entire request,
	// and the choices field will always be an empty array.
	// All other chunks will also include a usage field, but with a null value.
	IncludeUsage bool `json:"include_usage,omitempty"`
}

type Prediction struct {
	Content string `json:"content"`
	Type    string `json:"type"`
}

// 重点
// ChatCompletionRequest represents a request structure for chat completion API.
type ChatCompletionRequest struct {
	Model    string                  `json:"model"`
	Messages []ChatCompletionMessage `json:"messages"`
	// MaxTokens The maximum number of tokens that can be generated in the chat completion.
	// This value can be used to control costs for text generated via API.
	// This value is now deprecated in favor of max_completion_tokens, and is not compatible with o1 series models.
	// refs: https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens
	MaxTokens int `json:"max_tokens,omitempty"`
	// MaxCompletionTokens An upper bound for the number of tokens that can be generated for a completion,
	// including visible output tokens and reasoning tokens https://platform.openai.com/docs/guides/reasoning
	MaxCompletionTokens int                           `json:"max_completion_tokens,omitempty"`
	Temperature         float32                       `json:"temperature,omitempty"`
	TopP                float32                       `json:"top_p,omitempty"`
	N                   int                           `json:"n,omitempty"`
	Stream              bool                          `json:"stream,omitempty"`
	Stop                []string                      `json:"stop,omitempty"`
	PresencePenalty     float32                       `json:"presence_penalty,omitempty"`
	ResponseFormat      *ChatCompletionResponseFormat `json:"response_format,omitempty"`
	Seed                *int                          `json:"seed,omitempty"`
	FrequencyPenalty    float32                       `json:"frequency_penalty,omitempty"`
	// LogitBias is must be a token id string (specified by their token ID in the tokenizer), not a word string.
	// incorrect: `"logit_bias":{"You": 6}`, correct: `"logit_bias":{"1639": 6}`
	// refs: https://platform.openai.com/docs/api-reference/chat/create#chat/create-logit_bias
	LogitBias map[string]int `json:"logit_bias,omitempty"`
	// LogProbs indicates whether to return log probabilities of the output tokens or not.
	// If true, returns the log probabilities of each output token returned in the content of message.
	// This option is currently not available on the gpt-4-vision-preview model.
	LogProbs bool `json:"logprobs,omitempty"`
	// TopLogProbs is an integer between 0 and 5 specifying the number of most likely tokens to return at each
	// token position, each with an associated log probability.
	// logprobs must be set to true if this parameter is used.
	TopLogProbs int    `json:"top_logprobs,omitempty"`
	User        string `json:"user,omitempty"`
	// Deprecated: use Tools instead.
	Functions []FunctionDefinition `json:"functions,omitempty"`
	// Deprecated: use ToolChoice instead.
	FunctionCall any    `json:"function_call,omitempty"`
	Tools        []Tool `json:"tools,omitempty"`
	// This can be either a string or an ToolChoice object.
	ToolChoice any `json:"tool_choice,omitempty"`
	// Options for streaming response. Only set this when you set stream: true.
	StreamOptions *StreamOptions `json:"stream_options,omitempty"`
	// Disable the default behavior of parallel tool calls by setting it: false.
	ParallelToolCalls any `json:"parallel_tool_calls,omitempty"`
	// Store can be set to true to store the output of this completion request for use in distillations and evals.
	// https://platform.openai.com/docs/api-reference/chat/create#chat-create-store
	Store bool `json:"store,omitempty"`
	// Controls effort on reasoning for reasoning models. It can be set to "low", "medium", or "high".
	ReasoningEffort string `json:"reasoning_effort,omitempty"`
	// Metadata to store with the completion.
	Metadata map[string]string `json:"metadata,omitempty"`
	// Configuration for a predicted output.
	Prediction *Prediction `json:"prediction,omitempty"`
	// ChatTemplateKwargs provides a way to add non-standard parameters to the request body.
	// Additional kwargs to pass to the template renderer. Will be accessible by the chat template.
	// Such as think mode for qwen3. "chat_template_kwargs": {"enable_thinking": false}
	// https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes
	ChatTemplateKwargs map[string]any `json:"chat_template_kwargs,omitempty"`
}

type FunctionCall struct {
	Name string `json:"name,omitempty"`
	// call function with arguments in JSON format
	Arguments string `json:"arguments,omitempty"`
}

type Hate struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}
type SelfHarm struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}
type Sexual struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}
type Violence struct {
	Filtered bool   `json:"filtered"`
	Severity string `json:"severity,omitempty"`
}

type JailBreak struct {
	Filtered bool `json:"filtered"`
	Detected bool `json:"detected"`
}

type Profanity struct {
	Filtered bool `json:"filtered"`
	Detected bool `json:"detected"`
}

type ContentFilterResults struct {
	Hate      Hate      `json:"hate,omitempty"`
	SelfHarm  SelfHarm  `json:"self_harm,omitempty"`
	Sexual    Sexual    `json:"sexual,omitempty"`
	Violence  Violence  `json:"violence,omitempty"`
	JailBreak JailBreak `json:"jailbreak,omitempty"`
	Profanity Profanity `json:"profanity,omitempty"`
}

type ToolCall struct {
	// Index is not nil only in chat completion chunk object
	Index    *int         `json:"index,omitempty"`
	ID       string       `json:"id,omitempty"`
	Type     ToolType     `json:"type"`
	Function FunctionCall `json:"function"`
}

type ChatCompletionStreamChoiceDelta struct {
	Content      string        `json:"content,omitempty"`
	Role         string        `json:"role,omitempty"`
	FunctionCall *FunctionCall `json:"function_call,omitempty"`
	ToolCalls    []ToolCall    `json:"tool_calls,omitempty"`
	Refusal      string        `json:"refusal,omitempty"`

	// This property is used for the "reasoning" feature supported by deepseek-reasoner
	// which is not in the official documentation.
	// the doc from deepseek:
	// - https://api-docs.deepseek.com/api/create-chat-completion#responses
	ReasoningContent string `json:"reasoning_content,omitempty"`
}

type ChatCompletionTokenLogprobTopLogprob struct {
	Token   string  `json:"token"`
	Bytes   []int64 `json:"bytes"`
	Logprob float64 `json:"logprob"`
}

type ChatCompletionTokenLogprob struct {
	Token       string                                 `json:"token"`
	Bytes       []int64                                `json:"bytes,omitempty"`
	Logprob     float64                                `json:"logprob,omitempty"`
	TopLogprobs []ChatCompletionTokenLogprobTopLogprob `json:"top_logprobs"`
}

type ChatCompletionStreamChoice struct {
	Index                int                                 `json:"index"`
	Delta                ChatCompletionStreamChoiceDelta     `json:"delta"`
	Logprobs             *ChatCompletionStreamChoiceLogprobs `json:"logprobs,omitempty"`
	FinishReason         FinishReason                        `json:"finish_reason"`
	ContentFilterResults ContentFilterResults                `json:"content_filter_results,omitempty"`
}

type ChatCompletionStreamChoiceLogprobs struct {
	Content []ChatCompletionTokenLogprob `json:"content,omitempty"`
	Refusal []ChatCompletionTokenLogprob `json:"refusal,omitempty"`
}

type PromptAnnotation struct {
	PromptIndex          int                  `json:"prompt_index,omitempty"`
	ContentFilterResults ContentFilterResults `json:"content_filter_results,omitempty"`
}
type PromptFilterResult struct {
	Index                int                  `json:"index"`
	ContentFilterResults ContentFilterResults `json:"content_filter_results,omitempty"`
}

type TopLogProbs struct {
	Token   string  `json:"token"`
	LogProb float64 `json:"logprob"`
	Bytes   []byte  `json:"bytes,omitempty"`
}

// LogProb represents the probability information for a token.
type LogProb struct {
	Token   string  `json:"token"`
	LogProb float64 `json:"logprob"`
	Bytes   []byte  `json:"bytes,omitempty"` // Omitting the field if it is null
	// TopLogProbs is a list of the most likely tokens and their log probability, at this token position.
	// In rare cases, there may be fewer than the number of requested top_logprobs returned.
	TopLogProbs []TopLogProbs `json:"top_logprobs"`
}

// LogProbs is the top-level structure containing the log probability information.
type LogProbs struct {
	// Content is a list of message content tokens with log probability information.
	Content []LogProb `json:"content"`
}

type ChatCompletionChoice struct {
	Index   int                   `json:"index"`
	Message ChatCompletionMessage `json:"message"`
	// FinishReason
	// stop: API returned complete message,
	// or a message terminated by one of the stop sequences provided via the stop parameter
	// length: Incomplete model output due to max_tokens parameter or token limit
	// function_call: The model decided to call a function
	// content_filter: Omitted content due to a flag from our content filters
	// null: API response still in progress or incomplete
	FinishReason         FinishReason         `json:"finish_reason"`
	LogProbs             *LogProbs            `json:"logprobs,omitempty"`
	ContentFilterResults ContentFilterResults `json:"content_filter_results,omitempty"`
}

type ChatCompletionResponse struct {
	ID                  string                 `json:"id"`
	Object              string                 `json:"object"`
	Created             int64                  `json:"created"`
	Model               string                 `json:"model"`
	Choices             []ChatCompletionChoice `json:"choices"`
	Usage               Usage                  `json:"usage"`
	SystemFingerprint   string                 `json:"system_fingerprint"`
	PromptFilterResults []PromptFilterResult   `json:"prompt_filter_results,omitempty"`

	httpHeader
}

// 重点
type ChatCompletionStreamResponse struct {
	ID                  string                       `json:"id"`
	Object              string                       `json:"object"`
	Created             int64                        `json:"created"`
	Model               string                       `json:"model"`
	Choices             []ChatCompletionStreamChoice `json:"choices"`
	SystemFingerprint   string                       `json:"system_fingerprint"`
	PromptAnnotations   []PromptAnnotation           `json:"prompt_annotations,omitempty"`
	PromptFilterResults []PromptFilterResult         `json:"prompt_filter_results,omitempty"`
	// An optional field that will only be present when you set stream_options: {"include_usage": true} in your request.
	// When present, it contains a null value except for the last chunk which contains the token usage statistics
	// for the entire request.
	Usage *Usage `json:"usage,omitempty"`
}

type GPTChoice struct {
	Index        int        `json:"index"`
	Message      GptMessage `json:"message"`
	Logprobs     *Logprobs  `json:"logprobs"` // Assuming it could be nil.
	FinishReason string     `json:"finish_reason"`
}

type GptMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type Logprobs struct {
	// Define fields here if the structure is known; otherwise, use interface{} or leave undefined.
}

type Usage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

// createStreamMessage 创建流式消息
func CreateStreamMessage(chatId string, now int64, req *ChatCompletionRequest, fingerPrint string, content string, reasoningContent string) ChatCompletionStreamResponse {
	choice := ChatCompletionStreamChoice{
		Index: 0,
		Delta: ChatCompletionStreamChoiceDelta{
			Role:             ChatMessageRoleAssistant,
			Content:          content,
			ReasoningContent: reasoningContent,
		},
		ContentFilterResults: ContentFilterResults{},
		FinishReason:         FinishReasonNull,
	}

	return ChatCompletionStreamResponse{
		ID:                "chatcmpl-" + chatId,
		Object:            sseObject,
		Created:           now,
		Model:             req.Model,
		Choices:           []ChatCompletionStreamChoice{choice},
		SystemFingerprint: fingerPrint,
	}
}

func GpttoClaudeRequest(input []ChatCompletionMessage) []ClaudeMessageItem {
	msg := make([]ClaudeMessageItem, 0, len(input))
	for _, m := range input {
		if m.Role == "system" {
			m.Role = "assistant"
		}
		str := ""
		if m.Content == nil {
			m.Content = ""
		}
		str, ok := m.Content.(string)
		if ok {
			msg = append(msg, ClaudeMessageItem{
				Role:    m.Role,
				Content: []ClaudeMessageContent{{Type: "text", Text: str}},
			})
		}
		nmap, ok := m.Content.(map[string]string)
		if ok {
			msg = append(msg, ClaudeMessageItem{
				Role:    m.Role,
				Content: []ClaudeMessageContent{{Type: "text", Text: nmap["text"]}},
			})
		}
	}
	return msg
}

func MockGPTResponse() *ChatGPTResponse {

	c := GPTChoice{
		Index: 0,
		Message: GptMessage{
			Role:    "assistant",
			Content: "我可以帮助你完成许多任务，比如回答问题、提供建议、解决问题、生成内容（如文章、代码、总结等）、翻译语言、分析数据等等。如果你有具体的需求，比如需要查询信息、计算、写作辅助或工具使用，都可以告诉我！",
		},
		FinishReason: "stop",
	}

	return &ChatGPTResponse{
		ID:                "dfgdfg",
		Object:            "dfgdfg",
		Created:           123123123,
		Model:             "gpt-4.1",
		SystemFingerprint: "dfgdfg",
		Choices:           []GPTChoice{c},
		Usage:             Usage{},
	}
}

func GPTServer() {
	router := gin.Default()
	router.GET("/", func(c *gin.Context) {
		log.Println("收到根路径请求")
		c.String(http.StatusOK, "Ollama is running ok")
	})
	router.GET("/api/tags", getModels)
	router.GET("/api/models", GetGptModels)
	router.POST("/api/chat", chatHandlerSteam)
	router.POST("/v1/chat/completions", OpenaiHandler)

	log.Println("openai proxy server running at :" + strconv.Itoa(XConfig.OpenaiPort))
	router.Run(":" + strconv.Itoa(XConfig.OpenaiPort))
}

func GetGptModels(c *gin.Context) {
	models := make([]GptModel, 0)
	resp := GptModelListResp{}
	resp.Data = models

	for key := range XConfig.DifyAppMap {
		family := strings.Split(key, "-")[0]
		gpt := GptModel{
			ID:      key,
			Object:  "model",
			Created: 0,
			OwnedBy: family,
		}
		models = append(models, gpt)

	}
	for key := range XConfig.DifyAppMapProd {
		family := strings.Split(key, "-")[0]
		gpt := GptModel{
			ID:      key,
			Object:  "model",
			Created: 0,
			OwnedBy: family,
		}
		models = append(models, gpt)
	}
	c.JSON(http.StatusOK, resp)
}

func OpenaiHandlerSteam(c *gin.Context, input ChatCompletionRequest) {

	if model, ok := XConfig.Mapping[input.Model]; ok {
		input.Model = model
	}

	// dify 选择 url
	models := make([]string, 0)
	hasModels := make([]string, 0)
	for model := range XConfig.DifyAppMapProd {
		models = append(models, model)
		if model == input.Model {
			XConfig.IsProd = true
			break
		} else {
			hasModels = append(models, model)
		}
	}
	if len(hasModels) == len(models) {
		XConfig.IsProd = false
	}

	url := XConfig.APIURL
	if XConfig.IsProd {
		url = XConfig.APIURLProd
	}

	payload, err := GptGenRequest(&input)
	if err != nil {
		log.Println("Encode error:", err)
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to encode request" + err.Error()})
		return
	}

	req, err := http.NewRequest("POST", url, bytes.NewBuffer(payload))
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
	}

	if XConfig.ChatType == "dify" {
		log.Println("current DifyToken:", XConfig.DifyTokenMap[input.Model])
		req.Header.Set("Authorization", "Bearer "+XConfig.DifyTokenMap[input.Model])
	} else {
		req.Header.Set("Authorization", "Bearer "+XConfig.APIKey)
	}
	req.Header.Set("x-api-key", XConfig.APIKey)
	req.Header.Set("Content-Type", "application/json")

	// 发起请求
	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		log.Println("Request error:", err)
		c.JSON(http.StatusInternalServerError, gin.H{"error": "API request failed" + err.Error()})
		return
	}

	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		c.JSON(resp.StatusCode, gin.H{"error": "API request failed code is not 200", "data": string(body)})
		return
	}
	scanner := bufio.NewScanner(resp.Body)
	scanner.Buffer(make([]byte, InitialScannerBufferSize), MaxScannerBufferSize)
	scanner.Split(bufio.ScanLines)

	// 设置为流式响应
	c.Header("content-Type", "text/event-stream")
	// c.Header("content-Type", "application/x-ndjson")
	c.Header("cache-control", "no-cache")
	c.Header("Connection", "keep-alive")

	for scanner.Scan() {
		data := scanner.Text()
		if XConfig.Debug {
			println(data)
		}
		if len(data) < 6 {
			continue
		}
		if data[:5] != "data:" { //&& data[:6] != "[DONE]"
			continue
		}
		data = data[6:]

		sendLine, err := GptGenResponseStream([]byte(data), &input)
		log.Println("返回一行:", sendLine)
		if err != nil {
			log.Println("Encode error:", err)
			continue
		}
		if sendLine == "null" || sendLine == "" {
			continue
		}
		// re := []byte("data: " + str + "\n\n")
		outputMsg := fmt.Sprintf("data: %s\n\n", sendLine)
		info, err := c.Writer.WriteString(outputMsg)
		if err != nil {
			log.Println("Write error:", err)
			continue
		}
		log.Println("info:", info)
		c.Writer.Flush()
	}

	if err := scanner.Err(); err != nil {
		if err != io.EOF {
			log.Println("scanner error: " + err.Error())
		}
	}

}

func OpenaiHandler(c *gin.Context) {
	body, _ := c.GetRawData()
	var input ChatCompletionRequest
	err := json.Unmarshal(body, &input)
	if err != nil {
		log.Println("Bind "+string(body)+"error:", err)
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request" + err.Error()})
		return
	}
	msg := make([]ChatCompletionMessage, 0)
	for _, m := range input.Messages {
		switch content := m.Content.(type) {
		case string:
			// log.Println("string:", content)
			msg = append(msg, m)
		case map[string]string:
			// log.Println("map[string]string:", content["text"])
			// m.Content = content["text"]
			msg = append(msg, ChatCompletionMessage{Role: m.Role, Content: content["text"]})
		}
	}
	input.Messages = msg

	if XConfig != nil && XConfig.Mock {
		//c.Header("content-Type", "application/x-ndjson")
		c.Header("content-Type", "application/json")
		//c.Header("content-Type", "text/event-stream")
		c.Header("cache-control", "no-cache")
		c.Header("Connection", "keep-alive")
		//c.Writer.WriteHeader(http.StatusOK)
		//jsonStr, err := json.Marshal(MockGPTResponse())
		//if err != nil {
		//	log.Println("Encode error:", err)
		//}
		//_, _ = c.Writer.Write(jsonStr)
		c.JSON(200, MockGPTResponse())
		return
	}

	if input.Stream {
		OpenaiHandlerSteam(c, input)
		return
	}

	if model, ok := XConfig.Mapping[input.Model]; ok {
		input.Model = model
	}

	// dify 选择 url
	models := make([]string, 0)
	hasModels := make([]string, 0)
	for model := range XConfig.DifyAppMapProd {
		models = append(models, model)
		if model == input.Model {
			XConfig.IsProd = true
			break
		} else {
			hasModels = append(models, model)
		}
	}
	if len(hasModels) == len(models) {
		XConfig.IsProd = false
	}

	url := XConfig.APIURL
	if XConfig.IsProd {
		url = XConfig.APIURLProd
	}

	payload, err := GptGenRequest(&input)
	if err != nil {
		log.Println("Encode error:", err)
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to encode request" + err.Error()})
		return
	}

	req, err := http.NewRequest("POST", url, bytes.NewBuffer(payload))
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
	}

	if XConfig.ChatType == "dify" {
		log.Println("current DifyToken:", XConfig.DifyTokenMap[input.Model])
		req.Header.Set("Authorization", "Bearer "+XConfig.DifyTokenMap[input.Model])
	} else {
		req.Header.Set("Authorization", "Bearer "+XConfig.APIKey)
	}
	req.Header.Set("x-api-key", XConfig.APIKey)
	req.Header.Set("Content-Type", "application/json")

	// 发起请求
	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		log.Println("Request error:", err)
		c.JSON(http.StatusInternalServerError, gin.H{"error": "API request failed" + err.Error()})
		return
	}

	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		c.JSON(resp.StatusCode, gin.H{"error": "API request failed code is not 200", "data": string(body)})
		return
	}
	scanner := bufio.NewScanner(resp.Body)
	scanner.Buffer(make([]byte, InitialScannerBufferSize), MaxScannerBufferSize)
	scanner.Split(bufio.ScanLines)

	c.Header("content-Type", "application/json")
	c.Header("cache-control", "no-cache")
	c.Header("Connection", "keep-alive")

	for scanner.Scan() {
		data := scanner.Text()
		if XConfig.Debug {
			println(data)
		}
		if len(data) < 6 {
			continue
		}
		if data[:5] != "data:" { //&& data[:6] != "[DONE]"
			continue
		}
		data = data[6:]

		sendLine, err := GptGenResponse([]byte(data), &input)
		if err != nil {
			log.Println("Encode error:", err)
			continue
		}
		if sendLine == "null" || sendLine == "" {
			continue
		}
		c.JSON(http.StatusOK, sendLine)
	}

	if err := scanner.Err(); err != nil {
		if err != io.EOF {
			log.Println("scanner error: " + err.Error())
		}
	}

}

func GptGenRequest(input *ChatCompletionRequest) ([]byte, error) {
	if XConfig == nil {
		return nil, nil
	}
	switch XConfig.ChatType {
	case "dify":
		if XConfig.DifyTokenMap[input.Model] == "" {
			err := getDifyToken(input.Model)
			if err != nil {
				return nil, err
			}
		}
		log.Println("DifyTokenMap:", XConfig.DifyTokenMap)
		re := GptToDityRequest(input)
		return json.Marshal(re)
	case "claude":
		re := GpttoClaudeRequest(input.Messages)
		return json.Marshal(re)
	default:
		return nil, nil
	}

}

func GptGenResponse(input []byte, req *ChatCompletionRequest) (string, error) {
	if XConfig == nil {
		return "", nil
	}
	switch XConfig.ChatType {
	case "dify":
		return DifyToGptResponse(input, req)
	case "claude":
		// re, err := ClaudeBlockToOllamaResponse(input, req)
		// if err != nil {
		// 	return nil, nil
		// }
		// return json.Marshal(re)
		return "", nil
	default:
		return "", nil
	}

}

func GptGenResponseStream(input []byte, req *ChatCompletionRequest) (string, error) {
	if XConfig == nil {
		return "", nil
	}
	switch XConfig.ChatType {
	case "dify":
		return DifyToGptResponseStream(input, req)
	case "claude":
		// re, err := ClaudeBlockToOllamaResponse(input, req)
		// if err != nil {
		// 	return nil, nil
		// }
		// return json.Marshal(re)
		return "", nil
	default:
		return "", nil
	}

}
